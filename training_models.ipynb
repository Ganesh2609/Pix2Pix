{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchinfo \n",
    "\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from data import ImageNetForPIXGAN\n",
    "from trainer import train_models\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device agnostic code\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "\n",
    "LEARNING_RATE_G = 2e-4\n",
    "LEARNING_RATE_D = 2e-4\n",
    "L1_LAMBDA = 100\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 512\n",
    "\n",
    "GENERATOR_SAVE_PATH = 'Models/abacus_generator.pth'\n",
    "DISCRIMINATOR_SAVE_PATH = 'Models/abacus_discriminator.pth'\n",
    "RESULT_SAVE_PATH = 'Results/Abacus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the transforms \n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 123)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset and dataloaders \n",
    "\n",
    "root_dir = 'Abacus'\n",
    "data = ImageNetForPIXGAN(root=root_dir, transform=input_transform, target_transform=target_transform)\n",
    "dataloader = DataLoader(dataset=data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "len(data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model instances\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
    "\n",
    "generator = Generator(in_channels=1, out_channels=3).to(device)\n",
    "#initialize_weights(generator)\n",
    "\n",
    "discriminator = Discriminator(in_channels=4).to(device)\n",
    "#initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, optimizer and scaler\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "g_optimizer = torch.optim.Adam(params=generator.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A generator does not exist in the specified path... Creating the model and training it for the specified epochs...\n",
      "A discriminator does not exist in the specified path... Creating the model and training it for the specified epochs...\n"
     ]
    }
   ],
   "source": [
    "# Saving and loading models\n",
    "\n",
    "model_file = Path(GENERATOR_SAVE_PATH)\n",
    "if model_file.is_file():\n",
    "    generator.load_state_dict(torch.load(f=GENERATOR_SAVE_PATH))\n",
    "    print(\"A generator aleady exists... Loading that model and training it for the specified epochs...\")\n",
    "else:\n",
    "    print(\"A generator does not exist in the specified path... Creating the model and training it for the specified epochs...\")\n",
    "    \n",
    "model_file = Path(DISCRIMINATOR_SAVE_PATH)\n",
    "if model_file.is_file():\n",
    "    discriminator.load_state_dict(torch.load(f=DISCRIMINATOR_SAVE_PATH))\n",
    "    print(\"A discriminator aleady exists... Loading that model and training it for the specified epochs...\")\n",
    "else:\n",
    "    print(\"A discriminator does not exist in the specified path... Creating the model and training it for the specified epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/512]: 100%|██████████| 123/123 [02:20<00:00,  1.14s/it, Gen batch loss=19.3, Gen loss=24.4, Disc batch loss=0.0879, Disc loss=0.309, Real=0.889, Fake=0.041] \n",
      "Epoch [2/512]: 100%|██████████| 123/123 [02:18<00:00,  1.13s/it, Gen batch loss=21, Gen loss=20.8, Disc batch loss=0.0136, Disc loss=0.0708, Real=0.978, Fake=0.00468]  \n",
      "Epoch [3/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=20.8, Gen loss=19.6, Disc batch loss=0.00754, Disc loss=0.0584, Real=0.988, Fake=0.00316] \n",
      "Epoch [4/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=16.2, Gen loss=19.2, Disc batch loss=0.00347, Disc loss=0.0106, Real=0.996, Fake=0.00246] \n",
      "Epoch [5/512]: 100%|██████████| 123/123 [02:12<00:00,  1.07s/it, Gen batch loss=19.6, Gen loss=19.1, Disc batch loss=0.15, Disc loss=0.0161, Real=0.843, Fake=0.0677]      \n",
      "Epoch [6/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=13.5, Gen loss=17.8, Disc batch loss=0.00211, Disc loss=0.0232, Real=0.998, Fake=0.0017]   \n",
      "Epoch [7/512]: 100%|██████████| 123/123 [02:22<00:00,  1.16s/it, Gen batch loss=13.8, Gen loss=17.2, Disc batch loss=0.000538, Disc loss=0.00333, Real=0.999, Fake=0.000168]\n",
      "Epoch [8/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=18.6, Gen loss=17.2, Disc batch loss=0.00181, Disc loss=0.00369, Real=0.997, Fake=0.000551] \n",
      "Epoch [9/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=17.4, Gen loss=17, Disc batch loss=0.0438, Disc loss=0.0728, Real=0.947, Fake=0.023]        \n",
      "Epoch [10/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=17.7, Gen loss=16.7, Disc batch loss=0.0204, Disc loss=0.0229, Real=0.982, Fake=0.016]    \n",
      "Epoch [11/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=20.1, Gen loss=16.5, Disc batch loss=0.0018, Disc loss=0.00455, Real=0.997, Fake=0.000595]  \n",
      "Epoch [12/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=19.4, Gen loss=16.2, Disc batch loss=0.000467, Disc loss=0.00286, Real=0.999, Fake=0.000348]\n",
      "Epoch [13/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=15, Gen loss=16, Disc batch loss=0.0824, Disc loss=0.0411, Real=0.963, Fake=0.0798]        \n",
      "Epoch [14/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=10.7, Gen loss=15.9, Disc batch loss=0.0126, Disc loss=0.0278, Real=0.984, Fake=0.00872] \n",
      "Epoch [15/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=15.6, Gen loss=16.4, Disc batch loss=0.000976, Disc loss=0.015, Real=0.999, Fake=0.00071]  \n",
      "Epoch [16/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=16.1, Gen loss=16.2, Disc batch loss=0.00035, Disc loss=0.00668, Real=1, Fake=0.000254]     \n",
      "Epoch [17/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=11.7, Gen loss=15.8, Disc batch loss=0.000868, Disc loss=0.0257, Real=0.999, Fake=0.000635]\n",
      "Epoch [18/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=13.2, Gen loss=15.7, Disc batch loss=0.000576, Disc loss=0.00596, Real=0.999, Fake=9.53e-5] \n",
      "Epoch [19/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=14.9, Gen loss=15.6, Disc batch loss=0.000137, Disc loss=0.0012, Real=1, Fake=2.62e-5]      \n",
      "Epoch [20/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=19.1, Gen loss=15.6, Disc batch loss=0.000824, Disc loss=0.00796, Real=0.999, Fake=0.00031] \n",
      "Epoch [21/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=21.5, Gen loss=15.4, Disc batch loss=0.00203, Disc loss=0.0678, Real=0.997, Fake=0.00146]   \n",
      "Epoch [22/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=21.5, Gen loss=15.6, Disc batch loss=0.00297, Disc loss=0.0801, Real=0.996, Fake=0.00232]  \n",
      "Epoch [23/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=16.1, Gen loss=15.7, Disc batch loss=0.000438, Disc loss=0.0199, Real=0.999, Fake=0.000253]\n",
      "Epoch [24/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=14.8, Gen loss=15.3, Disc batch loss=0.00221, Disc loss=0.0685, Real=0.999, Fake=0.00313]  \n",
      "Epoch [25/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=19.5, Gen loss=15.4, Disc batch loss=0.00993, Disc loss=0.00505, Real=0.981, Fake=0.000186] \n",
      "Epoch [26/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=11.2, Gen loss=14.9, Disc batch loss=0.00356, Disc loss=0.00587, Real=0.994, Fake=0.00112] \n",
      "Epoch [27/512]: 100%|██████████| 123/123 [02:21<00:00,  1.15s/it, Gen batch loss=17, Gen loss=15.4, Disc batch loss=0.000411, Disc loss=0.049, Real=1, Fake=0.000327]        \n",
      "Epoch [28/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=18.1, Gen loss=15.2, Disc batch loss=0.000946, Disc loss=0.0153, Real=1, Fake=0.00141]     \n",
      "Epoch [29/512]: 100%|██████████| 123/123 [02:17<00:00,  1.11s/it, Gen batch loss=15.4, Gen loss=15, Disc batch loss=0.0013, Disc loss=0.0395, Real=0.998, Fake=0.000115]    \n",
      "Epoch [30/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=17.8, Gen loss=14.7, Disc batch loss=0.000455, Disc loss=0.0113, Real=0.999, Fake=0.00017] \n",
      "Epoch [31/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=11.6, Gen loss=14.2, Disc batch loss=0.00559, Disc loss=0.00188, Real=0.995, Fake=0.00633]  \n",
      "Epoch [32/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=13.6, Gen loss=14.2, Disc batch loss=0.000124, Disc loss=0.000854, Real=1, Fake=7e-5]        \n",
      "Epoch [33/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=12.2, Gen loss=14.5, Disc batch loss=0.000511, Disc loss=0.000973, Real=0.999, Fake=2.89e-5]\n",
      "Epoch [34/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=13.2, Gen loss=14.4, Disc batch loss=0.00199, Disc loss=0.00181, Real=0.998, Fake=0.0019]   \n",
      "Epoch [35/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=12.5, Gen loss=14.3, Disc batch loss=0.0018, Disc loss=0.0272, Real=1, Fake=0.00328]       \n",
      "Epoch [36/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=10.6, Gen loss=13.8, Disc batch loss=0.00345, Disc loss=0.00201, Real=0.995, Fake=0.00151]  \n",
      "Epoch [37/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=11.6, Gen loss=13.6, Disc batch loss=4.45e-5, Disc loss=0.000839, Real=1, Fake=4.47e-5]      \n",
      "Epoch [38/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=14.5, Gen loss=13.5, Disc batch loss=2.47e-5, Disc loss=0.000397, Real=1, Fake=2.6e-5]      \n",
      "Epoch [39/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=11.6, Gen loss=13.4, Disc batch loss=0.000109, Disc loss=0.000447, Real=1, Fake=8.92e-5]     \n",
      "Epoch [40/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=10.9, Gen loss=13.3, Disc batch loss=0.00072, Disc loss=0.000342, Real=0.999, Fake=5.63e-5] \n",
      "Epoch [41/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=9.89, Gen loss=13.7, Disc batch loss=0.000164, Disc loss=0.000635, Real=1, Fake=4.87e-5]    \n",
      "Epoch [42/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=15.1, Gen loss=13.1, Disc batch loss=0.00015, Disc loss=0.000358, Real=1, Fake=0.000158]    \n",
      "Epoch [43/512]: 100%|██████████| 123/123 [02:17<00:00,  1.11s/it, Gen batch loss=10.9, Gen loss=12.9, Disc batch loss=0.00312, Disc loss=0.000368, Real=0.995, Fake=0.000925] \n",
      "Epoch [44/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=9.84, Gen loss=13, Disc batch loss=0.000373, Disc loss=0.000148, Real=0.999, Fake=3.39e-5]  \n",
      "Epoch [45/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=10.1, Gen loss=12.8, Disc batch loss=0.000209, Disc loss=0.0002, Real=1, Fake=7.55e-5]      \n",
      "Epoch [46/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=15.7, Gen loss=12.7, Disc batch loss=0.000275, Disc loss=0.00025, Real=1, Fake=0.000511]   \n",
      "Epoch [47/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=12.5, Gen loss=12.7, Disc batch loss=2.47e-5, Disc loss=0.000157, Real=1, Fake=1.74e-5]     \n",
      "Epoch [48/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=11.5, Gen loss=12.6, Disc batch loss=6.79e-5, Disc loss=0.000124, Real=1, Fake=6.92e-5]    \n",
      "Epoch [49/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=14.6, Gen loss=12.5, Disc batch loss=4.83e-5, Disc loss=0.000308, Real=1, Fake=7.27e-5]      \n",
      "Epoch [50/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=16.1, Gen loss=12.2, Disc batch loss=2.33e-5, Disc loss=0.000212, Real=1, Fake=1.75e-5]     \n",
      "Epoch [51/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=18.3, Gen loss=12.3, Disc batch loss=0.000272, Disc loss=0.000198, Real=1, Fake=0.000389]  \n",
      "Epoch [52/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=8.5, Gen loss=12.3, Disc batch loss=8.02e-5, Disc loss=0.000347, Real=1, Fake=8.78e-5]      \n",
      "Epoch [53/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=13.1, Gen loss=12, Disc batch loss=0.000468, Disc loss=0.000212, Real=1, Fake=0.000819]     \n",
      "Epoch [54/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=14.1, Gen loss=12, Disc batch loss=6.41e-5, Disc loss=0.000113, Real=1, Fake=0.000117]      \n",
      "Epoch [55/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=11.3, Gen loss=12, Disc batch loss=3.7e-5, Disc loss=0.000106, Real=1, Fake=3.06e-5]        \n",
      "Epoch [56/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=11.9, Gen loss=12, Disc batch loss=4.24e-5, Disc loss=0.000115, Real=1, Fake=2.24e-5]      \n",
      "Epoch [57/512]: 100%|██████████| 123/123 [02:18<00:00,  1.12s/it, Gen batch loss=11.5, Gen loss=12.8, Disc batch loss=0.000791, Disc loss=0.0738, Real=0.999, Fake=0.000574]\n",
      "Epoch [58/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=19.5, Gen loss=11.8, Disc batch loss=0.0103, Disc loss=0.00241, Real=0.981, Fake=0.000163]  \n",
      "Epoch [59/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=17, Gen loss=11.5, Disc batch loss=0.00252, Disc loss=0.00389, Real=0.999, Fake=0.00414]   \n",
      "Epoch [60/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=12.3, Gen loss=11.4, Disc batch loss=0.0329, Disc loss=0.00345, Real=0.997, Fake=0.0433]    \n",
      "Epoch [61/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=18.2, Gen loss=11.4, Disc batch loss=0.000653, Disc loss=0.00157, Real=0.999, Fake=0.000412]\n",
      "Epoch [62/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=13.1, Gen loss=11.3, Disc batch loss=0.000195, Disc loss=0.00148, Real=1, Fake=6.52e-5]     \n",
      "Epoch [63/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=11.8, Gen loss=11.4, Disc batch loss=0.000394, Disc loss=0.000622, Real=1, Fake=0.000706]    \n",
      "Epoch [64/512]: 100%|██████████| 123/123 [02:11<00:00,  1.07s/it, Gen batch loss=15, Gen loss=11.7, Disc batch loss=0.00792, Disc loss=0.0011, Real=1, Fake=0.0153]          \n",
      "Epoch [65/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=7.99, Gen loss=11.3, Disc batch loss=0.000377, Disc loss=0.000452, Real=1, Fake=0.000667]    \n",
      "Epoch [66/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=13.9, Gen loss=11.1, Disc batch loss=0.000391, Disc loss=0.00034, Real=0.999, Fake=0.000137] \n",
      "Epoch [67/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=10.4, Gen loss=11.6, Disc batch loss=1.92, Disc loss=0.221, Real=0.558, Fake=0.597]        \n",
      "Epoch [68/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=8.15, Gen loss=11.1, Disc batch loss=1.01, Disc loss=0.658, Real=0.214, Fake=0.0617]\n",
      "Epoch [69/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=9.64, Gen loss=10.6, Disc batch loss=0.0935, Disc loss=0.184, Real=0.864, Fake=0.0141]  \n",
      "Epoch [70/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=12.2, Gen loss=10.5, Disc batch loss=0.0292, Disc loss=0.127, Real=0.989, Fake=0.0378] \n",
      "Epoch [71/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=10.3, Gen loss=10.4, Disc batch loss=0.0361, Disc loss=0.099, Real=0.994, Fake=0.0589]  \n",
      "Epoch [72/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=11.2, Gen loss=9.96, Disc batch loss=0.26, Disc loss=0.0584, Real=0.964, Fake=0.339]     \n",
      "Epoch [73/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.2, Gen loss=9.89, Disc batch loss=0.00903, Disc loss=0.0489, Real=0.991, Fake=0.00896] \n",
      "Epoch [74/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.77, Gen loss=9.85, Disc batch loss=0.00202, Disc loss=0.02, Real=0.999, Fake=0.00278]   \n",
      "Epoch [75/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=8.34, Gen loss=9.6, Disc batch loss=0.00624, Disc loss=0.0136, Real=0.997, Fake=0.0094]   \n",
      "Epoch [76/512]: 100%|██████████| 123/123 [02:11<00:00,  1.07s/it, Gen batch loss=11.3, Gen loss=9.8, Disc batch loss=0.191, Disc loss=0.112, Real=0.994, Fake=0.24]      \n",
      "Epoch [77/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=9.39, Gen loss=9.47, Disc batch loss=0.145, Disc loss=0.0517, Real=0.838, Fake=0.0772]   \n",
      "Epoch [78/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=10.2, Gen loss=9.71, Disc batch loss=0.0929, Disc loss=0.144, Real=0.876, Fake=0.0124]  \n",
      "Epoch [79/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=11.2, Gen loss=9.51, Disc batch loss=0.0264, Disc loss=0.106, Real=0.975, Fake=0.00706] \n",
      "Epoch [80/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=8.15, Gen loss=9.36, Disc batch loss=0.325, Disc loss=0.113, Real=0.998, Fake=0.41]     \n",
      "Epoch [81/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.69, Gen loss=9.33, Disc batch loss=0.0399, Disc loss=0.0952, Real=0.982, Fake=0.0569]   \n",
      "Epoch [82/512]: 100%|██████████| 123/123 [02:18<00:00,  1.13s/it, Gen batch loss=9.27, Gen loss=9.26, Disc batch loss=0.0339, Disc loss=0.122, Real=0.967, Fake=0.0193]   \n",
      "Epoch [83/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=10.8, Gen loss=9.14, Disc batch loss=0.449, Disc loss=0.118, Real=0.935, Fake=0.475]    \n",
      "Epoch [84/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=13.3, Gen loss=9.23, Disc batch loss=0.202, Disc loss=0.225, Real=0.985, Fake=0.276]    \n",
      "Epoch [85/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=6.5, Gen loss=9.15, Disc batch loss=1.5, Disc loss=0.215, Real=0.997, Fake=0.886]       \n",
      "Epoch [86/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=10.8, Gen loss=9, Disc batch loss=0.0828, Disc loss=0.161, Real=0.973, Fake=0.12]       \n",
      "Epoch [87/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.71, Gen loss=8.58, Disc batch loss=0.0356, Disc loss=0.182, Real=0.98, Fake=0.0458]  \n",
      "Epoch [88/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=11.2, Gen loss=8.99, Disc batch loss=0.23, Disc loss=0.242, Real=0.981, Fake=0.318]     \n",
      "Epoch [89/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.95, Gen loss=8.79, Disc batch loss=0.0188, Disc loss=0.262, Real=0.993, Fake=0.0296]  \n",
      "Epoch [90/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=10.8, Gen loss=8.69, Disc batch loss=0.116, Disc loss=0.179, Real=0.838, Fake=0.0353]   \n",
      "Epoch [91/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=9.16, Gen loss=8.7, Disc batch loss=0.0654, Disc loss=0.101, Real=0.995, Fake=0.105]     \n",
      "Epoch [92/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=11.2, Gen loss=8.82, Disc batch loss=0.0523, Disc loss=0.123, Real=0.959, Fake=0.0543]  \n",
      "Epoch [93/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.75, Gen loss=8.69, Disc batch loss=0.842, Disc loss=0.119, Real=0.225, Fake=0.0037]    \n",
      "Epoch [94/512]: 100%|██████████| 123/123 [02:19<00:00,  1.13s/it, Gen batch loss=11, Gen loss=8.68, Disc batch loss=1.99, Disc loss=0.13, Real=0.973, Fake=0.953]        \n",
      "Epoch [95/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=16, Gen loss=8.91, Disc batch loss=0.0115, Disc loss=0.0427, Real=0.992, Fake=0.0146]     \n",
      "Epoch [96/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=8.84, Gen loss=8.84, Disc batch loss=0.00307, Disc loss=0.156, Real=0.996, Fake=0.00208]\n",
      "Epoch [97/512]: 100%|██████████| 123/123 [02:18<00:00,  1.12s/it, Gen batch loss=9.14, Gen loss=8.9, Disc batch loss=0.162, Disc loss=0.176, Real=0.773, Fake=0.00274]   \n",
      "Epoch [98/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=9.65, Gen loss=8.59, Disc batch loss=0.0395, Disc loss=0.0251, Real=0.934, Fake=0.00631]  \n",
      "Epoch [99/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.84, Gen loss=8.86, Disc batch loss=0.0091, Disc loss=0.0342, Real=0.998, Fake=0.0154]  \n",
      "Epoch [100/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=7.44, Gen loss=8.68, Disc batch loss=0.0272, Disc loss=0.0776, Real=0.979, Fake=0.0306]  \n",
      "Epoch [101/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=6.74, Gen loss=8.25, Disc batch loss=0.101, Disc loss=0.17, Real=0.891, Fake=0.0196]    \n",
      "Epoch [102/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.89, Gen loss=7.99, Disc batch loss=0.00622, Disc loss=0.0809, Real=0.99, Fake=0.00232] \n",
      "Epoch [103/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=7.53, Gen loss=8, Disc batch loss=0.00945, Disc loss=0.0291, Real=0.988, Fake=0.00495]    \n",
      "Epoch [104/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=8.57, Gen loss=8.02, Disc batch loss=0.00674, Disc loss=0.0629, Real=0.989, Fake=0.00123] \n",
      "Epoch [105/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=6.53, Gen loss=7.76, Disc batch loss=0.507, Disc loss=0.0273, Real=0.518, Fake=0.00379]    \n",
      "Epoch [106/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.91, Gen loss=7.95, Disc batch loss=0.0139, Disc loss=0.0337, Real=0.986, Fake=0.013]    \n",
      "Epoch [107/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.07, Gen loss=7.79, Disc batch loss=0.0513, Disc loss=0.0165, Real=0.911, Fake=0.00173]  \n",
      "Epoch [108/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=8.01, Gen loss=7.94, Disc batch loss=0.0155, Disc loss=0.0348, Real=0.987, Fake=0.0172]    \n",
      "Epoch [109/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=8.06, Gen loss=7.76, Disc batch loss=0.0239, Disc loss=0.0307, Real=1, Fake=0.0428]       \n",
      "Epoch [110/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=5.5, Gen loss=7.47, Disc batch loss=0.000335, Disc loss=0.0048, Real=1, Fake=0.000342]      \n",
      "Epoch [111/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=9.49, Gen loss=7.42, Disc batch loss=0.06, Disc loss=0.00436, Real=0.897, Fake=0.000742]    \n",
      "Epoch [112/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=7.4, Gen loss=7.59, Disc batch loss=0.0155, Disc loss=0.0736, Real=0.972, Fake=0.00164]    \n",
      "Epoch [113/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=4.8, Gen loss=7.53, Disc batch loss=0.00456, Disc loss=0.0044, Real=0.998, Fake=0.00654]    \n",
      "Epoch [114/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=7.48, Gen loss=7.42, Disc batch loss=0.00486, Disc loss=0.00187, Real=0.993, Fake=0.00235]  \n",
      "Epoch [115/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=8.51, Gen loss=7.45, Disc batch loss=0.00288, Disc loss=0.00225, Real=0.999, Fake=0.00453]  \n",
      "Epoch [116/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=7.04, Gen loss=7.35, Disc batch loss=0.000945, Disc loss=0.00298, Real=0.998, Fake=0.000196]\n",
      "Epoch [117/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=6.89, Gen loss=7.31, Disc batch loss=0.000452, Disc loss=0.00155, Real=1, Fake=0.000501]    \n",
      "Epoch [118/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.43, Gen loss=7.15, Disc batch loss=0.000519, Disc loss=0.00221, Real=1, Fake=0.000853]    \n",
      "Epoch [119/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=7.57, Gen loss=7.19, Disc batch loss=0.00617, Disc loss=0.00122, Real=1, Fake=0.0116]       \n",
      "Epoch [120/512]: 100%|██████████| 123/123 [02:15<00:00,  1.11s/it, Gen batch loss=9.11, Gen loss=7.3, Disc batch loss=0.00342, Disc loss=0.0018, Real=0.997, Fake=0.00383]    \n",
      "Epoch [121/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.3, Gen loss=7.43, Disc batch loss=0.0112, Disc loss=0.00371, Real=0.983, Fake=0.000659]  \n",
      "Epoch [122/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=8.83, Gen loss=7.41, Disc batch loss=0.00161, Disc loss=0.0135, Real=0.998, Fake=0.00155]  \n",
      "Epoch [123/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=6.53, Gen loss=7.53, Disc batch loss=0.0678, Disc loss=0.0469, Real=0.886, Fake=0.000342]  \n",
      "Epoch [124/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.9, Gen loss=6.97, Disc batch loss=0.00135, Disc loss=0.0143, Real=0.998, Fake=0.00104]   \n",
      "Epoch [125/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=5.23, Gen loss=7, Disc batch loss=0.00306, Disc loss=0.00365, Real=0.996, Fake=0.00178]     \n",
      "Epoch [126/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=17.2, Gen loss=6.82, Disc batch loss=0.00886, Disc loss=0.00231, Real=0.989, Fake=0.000651] \n",
      "Epoch [127/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=6.46, Gen loss=6.75, Disc batch loss=0.00147, Disc loss=0.00108, Real=0.998, Fake=0.00103]  \n",
      "Epoch [128/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=5.82, Gen loss=6.77, Disc batch loss=0.000987, Disc loss=0.000592, Real=0.998, Fake=8.37e-5]\n",
      "Epoch [129/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=9.4, Gen loss=6.85, Disc batch loss=0.000137, Disc loss=0.000845, Real=1, Fake=0.000135]     \n",
      "Epoch [130/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=7.34, Gen loss=6.65, Disc batch loss=0.000271, Disc loss=0.00104, Real=1, Fake=0.00039]     \n",
      "Epoch [131/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=7.68, Gen loss=6.67, Disc batch loss=0.000788, Disc loss=0.00154, Real=1, Fake=0.00126]     \n",
      "Epoch [132/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.98, Gen loss=6.48, Disc batch loss=0.000299, Disc loss=0.00077, Real=1, Fake=0.000541]     \n",
      "Epoch [133/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=9.78, Gen loss=6.45, Disc batch loss=0.000462, Disc loss=0.000495, Real=1, Fake=0.000533]    \n",
      "Epoch [134/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=6.18, Gen loss=6.94, Disc batch loss=5.9e-5, Disc loss=0.000439, Real=1, Fake=2.88e-5]       \n",
      "Epoch [135/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=6.68, Gen loss=6.46, Disc batch loss=0.000227, Disc loss=0.000429, Real=1, Fake=0.000411]    \n",
      "Epoch [136/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=5.95, Gen loss=7.35, Disc batch loss=0.000836, Disc loss=0.144, Real=1, Fake=0.00123]      \n",
      "Epoch [137/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=9.72, Gen loss=6.86, Disc batch loss=0.00139, Disc loss=0.0267, Real=0.998, Fake=0.00127]  \n",
      "Epoch [138/512]: 100%|██████████| 123/123 [02:15<00:00,  1.11s/it, Gen batch loss=7.91, Gen loss=7.26, Disc batch loss=0.00575, Disc loss=0.139, Real=0.998, Fake=0.00942]   \n",
      "Epoch [139/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=6.86, Gen loss=7, Disc batch loss=0.00201, Disc loss=0.0637, Real=0.999, Fake=0.00316]   \n",
      "Epoch [140/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.08, Gen loss=6.78, Disc batch loss=0.00185, Disc loss=0.00559, Real=0.998, Fake=0.00148]  \n",
      "Epoch [141/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=6.33, Gen loss=6.62, Disc batch loss=0.0118, Disc loss=0.00538, Real=1, Fake=0.0215]       \n",
      "Epoch [142/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.24, Gen loss=6.42, Disc batch loss=0.00157, Disc loss=0.00306, Real=0.998, Fake=0.00129]  \n",
      "Epoch [143/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.46, Gen loss=6.41, Disc batch loss=0.00189, Disc loss=0.00167, Real=1, Fake=0.00336]      \n",
      "Epoch [144/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=6.15, Gen loss=6.28, Disc batch loss=0.000444, Disc loss=0.00167, Real=0.999, Fake=0.000237]\n",
      "Epoch [145/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=7.18, Gen loss=6.35, Disc batch loss=0.00311, Disc loss=0.00209, Real=0.994, Fake=0.000279] \n",
      "Epoch [146/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=6.18, Gen loss=6.19, Disc batch loss=0.0036, Disc loss=0.002, Real=0.999, Fake=0.0064]      \n",
      "Epoch [147/512]: 100%|██████████| 123/123 [02:20<00:00,  1.14s/it, Gen batch loss=5.88, Gen loss=6.12, Disc batch loss=0.00973, Disc loss=0.00194, Real=0.995, Fake=0.0133]   \n",
      "Epoch [148/512]: 100%|██████████| 123/123 [02:18<00:00,  1.13s/it, Gen batch loss=6.74, Gen loss=6.16, Disc batch loss=0.000713, Disc loss=0.00166, Real=0.999, Fake=0.000227]\n",
      "Epoch [149/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.35, Gen loss=6.39, Disc batch loss=9.58e-5, Disc loss=0.000866, Real=1, Fake=8.63e-5]      \n",
      "Epoch [150/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=7.14, Gen loss=6.23, Disc batch loss=0.0125, Disc loss=0.000932, Real=0.98, Fake=0.000769]   \n",
      "Epoch [151/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=4.99, Gen loss=6.15, Disc batch loss=0.00033, Disc loss=0.00218, Real=1, Fake=0.000398]     \n",
      "Epoch [152/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.23, Gen loss=7.31, Disc batch loss=0.00783, Disc loss=0.24, Real=0.988, Fake=0.00219]    \n",
      "Epoch [153/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=9.22, Gen loss=8.27, Disc batch loss=0.00101, Disc loss=0.00177, Real=0.999, Fake=0.00149]  \n",
      "Epoch [154/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=13.4, Gen loss=8.15, Disc batch loss=0.000412, Disc loss=0.00155, Real=1, Fake=0.000584]    \n",
      "Epoch [155/512]: 100%|██████████| 123/123 [02:13<00:00,  1.08s/it, Gen batch loss=6.8, Gen loss=7.8, Disc batch loss=0.00469, Disc loss=0.00105, Real=0.997, Fake=0.00544]    \n",
      "Epoch [156/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=6.89, Gen loss=6.9, Disc batch loss=0.000958, Disc loss=0.00167, Real=1, Fake=0.00144]      \n",
      "Epoch [157/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=7.53, Gen loss=6.92, Disc batch loss=0.000497, Disc loss=0.00201, Real=0.999, Fake=0.000119]\n",
      "Epoch [158/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.86, Gen loss=8.49, Disc batch loss=0.000161, Disc loss=0.000347, Real=1, Fake=2.2e-5]      \n",
      "Epoch [159/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.66, Gen loss=7.86, Disc batch loss=0.000109, Disc loss=0.000249, Real=1, Fake=5.56e-5]    \n",
      "Epoch [160/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.29, Gen loss=7.73, Disc batch loss=0.001, Disc loss=0.000179, Real=0.999, Fake=0.000782]  \n",
      "Epoch [161/512]: 100%|██████████| 123/123 [02:14<00:00,  1.10s/it, Gen batch loss=8.08, Gen loss=7.7, Disc batch loss=0.000914, Disc loss=0.000149, Real=0.998, Fake=5.01e-5]  \n",
      "Epoch [162/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.54, Gen loss=7.6, Disc batch loss=5.71e-5, Disc loss=0.000138, Real=1, Fake=5.13e-5]      \n",
      "Epoch [163/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=8.15, Gen loss=7.66, Disc batch loss=7.03e-5, Disc loss=0.000114, Real=1, Fake=1.8e-5]      \n",
      "Epoch [164/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.25, Gen loss=7.78, Disc batch loss=7.96e-5, Disc loss=0.000136, Real=1, Fake=5.01e-5]      \n",
      "Epoch [165/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=8.61, Gen loss=7.37, Disc batch loss=0.000118, Disc loss=0.00019, Real=1, Fake=0.000119]    \n",
      "Epoch [166/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.97, Gen loss=6.99, Disc batch loss=0.000134, Disc loss=0.00047, Real=1, Fake=0.000238]    \n",
      "Epoch [167/512]: 100%|██████████| 123/123 [02:13<00:00,  1.09s/it, Gen batch loss=6.44, Gen loss=6.59, Disc batch loss=0.000539, Disc loss=0.000279, Real=1, Fake=0.000606]   \n",
      "Epoch [168/512]: 100%|██████████| 123/123 [02:18<00:00,  1.13s/it, Gen batch loss=6.25, Gen loss=6.18, Disc batch loss=0.0032, Disc loss=0.000638, Real=0.996, Fake=0.00127]   \n",
      "Epoch [169/512]: 100%|██████████| 123/123 [02:17<00:00,  1.11s/it, Gen batch loss=6.18, Gen loss=6.14, Disc batch loss=0.000335, Disc loss=0.000649, Real=1, Fake=0.00051]    \n",
      "Epoch [170/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=6.9, Gen loss=6.06, Disc batch loss=0.00103, Disc loss=0.00158, Real=0.999, Fake=0.00125]   \n",
      "Epoch [171/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=8.3, Gen loss=5.96, Disc batch loss=0.000262, Disc loss=0.000551, Real=1, Fake=0.000112]    \n",
      "Epoch [172/512]: 100%|██████████| 123/123 [02:12<00:00,  1.07s/it, Gen batch loss=8.38, Gen loss=6.26, Disc batch loss=0.000771, Disc loss=0.000643, Real=0.999, Fake=0.000388]\n",
      "Epoch [173/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=6, Gen loss=6.24, Disc batch loss=0.000161, Disc loss=0.000357, Real=1, Fake=0.000144]       \n",
      "Epoch [174/512]: 100%|██████████| 123/123 [02:10<00:00,  1.06s/it, Gen batch loss=5.82, Gen loss=5.81, Disc batch loss=0.000507, Disc loss=0.000684, Real=1, Fake=0.000814]    \n",
      "Epoch [175/512]: 100%|██████████| 123/123 [02:12<00:00,  1.08s/it, Gen batch loss=6.93, Gen loss=7.28, Disc batch loss=0.00163, Disc loss=0.146, Real=1, Fake=0.00284]       \n",
      "Epoch [176/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.16, Gen loss=6.82, Disc batch loss=0.00323, Disc loss=0.161, Real=0.997, Fake=0.00343]  \n",
      "Epoch [177/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.44, Gen loss=6.25, Disc batch loss=0.000672, Disc loss=0.00385, Real=0.999, Fake=0.000204]\n",
      "Epoch [178/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=4.7, Gen loss=6, Disc batch loss=0.00413, Disc loss=0.00207, Real=0.999, Fake=0.0068]       \n",
      "Epoch [179/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=6.01, Gen loss=5.81, Disc batch loss=0.00487, Disc loss=0.00202, Real=0.996, Fake=0.00549] \n",
      "Epoch [180/512]: 100%|██████████| 123/123 [02:15<00:00,  1.10s/it, Gen batch loss=7.1, Gen loss=5.94, Disc batch loss=0.00223, Disc loss=0.007, Real=0.998, Fake=0.00262]     \n",
      "Epoch [181/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.48, Gen loss=5.64, Disc batch loss=0.00113, Disc loss=0.00434, Real=0.999, Fake=0.000797]\n",
      "Epoch [182/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.71, Gen loss=6.12, Disc batch loss=0.0014, Disc loss=0.00389, Real=1, Fake=0.00254]       \n",
      "Epoch [183/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=7.41, Gen loss=6.46, Disc batch loss=3.91e-5, Disc loss=0.0271, Real=1, Fake=5.18e-5]      \n",
      "Epoch [184/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.06, Gen loss=7.7, Disc batch loss=0.000104, Disc loss=0.000536, Real=1, Fake=6.01e-5]      \n",
      "Epoch [185/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=9.89, Gen loss=7.81, Disc batch loss=6.46e-5, Disc loss=0.000357, Real=1, Fake=7.12e-5]     \n",
      "Epoch [186/512]: 100%|██████████| 123/123 [02:17<00:00,  1.11s/it, Gen batch loss=7.7, Gen loss=7.42, Disc batch loss=0.000184, Disc loss=0.000539, Real=1, Fake=0.000324]    \n",
      "Epoch [187/512]: 100%|██████████| 123/123 [02:18<00:00,  1.13s/it, Gen batch loss=4.29, Gen loss=6.49, Disc batch loss=0.000106, Disc loss=0.000227, Real=1, Fake=0.000171]    \n",
      "Epoch [188/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=7.87, Gen loss=5.71, Disc batch loss=0.000579, Disc loss=0.000408, Real=0.999, Fake=0.00055]\n",
      "Epoch [189/512]: 100%|██████████| 123/123 [02:19<00:00,  1.14s/it, Gen batch loss=5.15, Gen loss=6.1, Disc batch loss=0.000904, Disc loss=0.00199, Real=1, Fake=0.00163]      \n",
      "Epoch [190/512]: 100%|██████████| 123/123 [02:17<00:00,  1.12s/it, Gen batch loss=4.81, Gen loss=5.74, Disc batch loss=0.0184, Disc loss=0.00196, Real=0.967, Fake=0.000822]  \n",
      "Epoch [191/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=7.51, Gen loss=6.29, Disc batch loss=0.00187, Disc loss=0.0999, Real=0.998, Fake=0.00156]  \n",
      "Epoch [192/512]: 100%|██████████| 123/123 [02:18<00:00,  1.12s/it, Gen batch loss=5.41, Gen loss=5.99, Disc batch loss=0.00131, Disc loss=0.00253, Real=1, Fake=0.00243]      \n",
      "Epoch [193/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=5.02, Gen loss=5.59, Disc batch loss=0.000605, Disc loss=0.00148, Real=0.999, Fake=0.00023] \n",
      "Epoch [194/512]: 100%|██████████| 123/123 [02:16<00:00,  1.11s/it, Gen batch loss=4.11, Gen loss=5.49, Disc batch loss=0.00178, Disc loss=0.00248, Real=0.999, Fake=0.00211]  \n",
      "Epoch [195/512]: 100%|██████████| 123/123 [02:14<00:00,  1.09s/it, Gen batch loss=4.1, Gen loss=5.48, Disc batch loss=0.000619, Disc loss=0.00196, Real=1, Fake=0.000802]    \n",
      "Epoch [196/512]: 100%|██████████| 123/123 [02:18<00:00,  1.12s/it, Gen batch loss=3.89, Gen loss=5.36, Disc batch loss=0.00119, Disc loss=0.000794, Real=0.999, Fake=0.00186]  \n",
      "Epoch [197/512]: 100%|██████████| 123/123 [02:34<00:00,  1.26s/it, Gen batch loss=7.76, Gen loss=5.24, Disc batch loss=0.000319, Disc loss=0.000458, Real=1, Fake=0.000595]    \n",
      "Epoch [198/512]: 100%|██████████| 123/123 [02:52<00:00,  1.40s/it, Gen batch loss=4.58, Gen loss=5.39, Disc batch loss=0.000114, Disc loss=0.000608, Real=1, Fake=0.000193]    \n",
      "Epoch [199/512]: 100%|██████████| 123/123 [03:15<00:00,  1.59s/it, Gen batch loss=7.24, Gen loss=5.86, Disc batch loss=0.000314, Disc loss=0.00356, Real=1, Fake=0.000443]    \n",
      "Epoch [200/512]: 100%|██████████| 123/123 [03:21<00:00,  1.64s/it, Gen batch loss=5.55, Gen loss=5.34, Disc batch loss=8.55e-5, Disc loss=0.000996, Real=1, Fake=4.67e-5]    \n",
      "Epoch [201/512]: 100%|██████████| 123/123 [03:22<00:00,  1.65s/it, Gen batch loss=6.68, Gen loss=5.4, Disc batch loss=0.000175, Disc loss=0.000408, Real=1, Fake=0.000178]    \n",
      "Epoch [202/512]: 100%|██████████| 123/123 [03:24<00:00,  1.66s/it, Gen batch loss=5.7, Gen loss=5.24, Disc batch loss=9.39e-5, Disc loss=0.000356, Real=1, Fake=0.000156]      \n",
      "Epoch [203/512]: 100%|██████████| 123/123 [03:34<00:00,  1.74s/it, Gen batch loss=6.24, Gen loss=5.3, Disc batch loss=0.000453, Disc loss=0.000583, Real=1, Fake=0.000471]    \n",
      "Epoch [204/512]: 100%|██████████| 123/123 [03:36<00:00,  1.76s/it, Gen batch loss=5.14, Gen loss=5.28, Disc batch loss=0.000516, Disc loss=0.000338, Real=1, Fake=0.000871]   \n",
      "Epoch [205/512]: 100%|██████████| 123/123 [03:40<00:00,  1.79s/it, Gen batch loss=5.64, Gen loss=5.32, Disc batch loss=8.96e-5, Disc loss=0.000406, Real=1, Fake=0.000111]    \n",
      "Epoch [206/512]: 100%|██████████| 123/123 [03:43<00:00,  1.81s/it, Gen batch loss=8.25, Gen loss=6.86, Disc batch loss=0.0263, Disc loss=0.203, Real=0.991, Fake=0.0365]      \n",
      "Epoch [207/512]: 100%|██████████| 123/123 [04:04<00:00,  1.99s/it, Gen batch loss=8.77, Gen loss=6.76, Disc batch loss=0.00454, Disc loss=0.114, Real=0.991, Fake=0.000294]\n",
      "Epoch [208/512]: 100%|██████████| 123/123 [04:18<00:00,  2.11s/it, Gen batch loss=6.05, Gen loss=6.11, Disc batch loss=0.000746, Disc loss=0.0136, Real=1, Fake=0.00138]     \n",
      "Epoch [209/512]: 100%|██████████| 123/123 [04:30<00:00,  2.20s/it, Gen batch loss=5.63, Gen loss=5.93, Disc batch loss=0.00355, Disc loss=0.00217, Real=0.993, Fake=0.000274] \n",
      "Epoch [210/512]: 100%|██████████| 123/123 [04:36<00:00,  2.25s/it, Gen batch loss=6.83, Gen loss=5.42, Disc batch loss=0.00152, Disc loss=0.00226, Real=0.998, Fake=0.00111]  \n",
      "Epoch [211/512]: 100%|██████████| 123/123 [04:58<00:00,  2.42s/it, Gen batch loss=5.46, Gen loss=5.65, Disc batch loss=0.000183, Disc loss=0.00145, Real=1, Fake=0.000142]    \n",
      "Epoch [212/512]: 100%|██████████| 123/123 [04:59<00:00,  2.44s/it, Gen batch loss=5.37, Gen loss=5.27, Disc batch loss=0.00563, Disc loss=0.0015, Real=0.999, Fake=0.0103]    \n",
      "Epoch [213/512]: 100%|██████████| 123/123 [04:31<00:00,  2.21s/it, Gen batch loss=5.35, Gen loss=5.25, Disc batch loss=0.000525, Disc loss=0.000673, Real=1, Fake=0.000795]    \n",
      "Epoch [214/512]: 100%|██████████| 123/123 [04:41<00:00,  2.29s/it, Gen batch loss=4.96, Gen loss=5.15, Disc batch loss=0.000292, Disc loss=0.000797, Real=1, Fake=0.000228]    \n",
      "Epoch [215/512]: 100%|██████████| 123/123 [04:48<00:00,  2.34s/it, Gen batch loss=6.44, Gen loss=5.39, Disc batch loss=0.000468, Disc loss=0.000625, Real=1, Fake=0.000858]    \n",
      "Epoch [216/512]: 100%|██████████| 123/123 [04:55<00:00,  2.40s/it, Gen batch loss=5.07, Gen loss=5.54, Disc batch loss=7.74e-5, Disc loss=0.000625, Real=1, Fake=0.000136]     \n",
      "Epoch [217/512]: 100%|██████████| 123/123 [05:09<00:00,  2.51s/it, Gen batch loss=5.12, Gen loss=5.39, Disc batch loss=0.000691, Disc loss=0.000789, Real=1, Fake=0.00136]     \n",
      "Epoch [218/512]:   7%|▋         | 9/123 [00:35<07:28,  3.94s/it, Gen batch loss=5.47, Gen loss=5.27, Disc batch loss=9.8e-5, Disc loss=0.000238, Real=1, Fake=0.000101]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Actual training\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbce_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbce_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43ml1_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL1_LAMBDA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mg_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43md_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m             \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m             \u001b[49m\u001b[43mg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERATOR_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m             \u001b[49m\u001b[43md_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISCRIMINATOR_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m             \u001b[49m\u001b[43mresult_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESULT_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[1;32mf:\\Ganesh\\Amrita\\Subjects\\Sem 5\\Deep Learning\\My works\\GAN\\Pix2Pix\\trainer.py:108\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(generator, discriminator, dataset, dataloader, bce_loss, l1_loss, l1_lambda, g_optimizer, d_optimizer, g_scaler, d_scaler, device, NUM_EPOCHS, g_path, d_path, result_path)\u001b[0m\n\u001b[0;32m    104\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m--> 108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisc_batch_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_batch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Ganesh\\Amrita\\Subjects\\Sem 5\\Deep Learning\\My works\\GAN\\Pix2Pix\\data.py:31\u001b[0m, in \u001b[0;36mImageNetForPIXGAN.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     30\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[1;32m---> 31\u001b[0m input_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m target_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_transform:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\PIL\\Image.py:3442\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3439\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3440\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3442\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3444\u001b[0m preinit()\n\u001b[0;32m   3446\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actual training\n",
    "\n",
    "train_models(generator=generator,\n",
    "             discriminator=discriminator,\n",
    "             dataset=data,\n",
    "             dataloader=dataloader,\n",
    "             bce_loss=bce_loss,\n",
    "             l1_loss=l1_loss,\n",
    "             l1_lambda=L1_LAMBDA,\n",
    "             g_optimizer=g_optimizer,\n",
    "             d_optimizer=d_optimizer,\n",
    "             g_scaler=g_scaler,\n",
    "             d_scaler=d_scaler,\n",
    "             device=device,\n",
    "             NUM_EPOCHS=NUM_EPOCHS,\n",
    "             g_path=GENERATOR_SAVE_PATH,\n",
    "             d_path=DISCRIMINATOR_SAVE_PATH,\n",
    "             result_path=RESULT_SAVE_PATH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving models\n",
    "\n",
    "torch.save(obj=generator.state_dict(), f=GENERATOR_SAVE_PATH)\n",
    "torch.save(obj=discriminator.state_dict(), f=DISCRIMINATOR_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
